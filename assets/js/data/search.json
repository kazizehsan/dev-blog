[ { "title": "An introduction to GitHub Actions", "url": "/posts/intro-github-actions/", "categories": "Guides", "tags": "cicd, github, github-actions, ecr, ecs-task-definition", "date": "2023-12-03 23:30:00 +0600", "snippet": "Photo by Mike Benna on UnsplashGitHub Actions is free to use for standard GitHub-hosted runners in public repositories.Concepts Each job runs-on in a fresh virtual environment Each job consists of steps Each step uses another prebuilt action or can run some commands In an action file, each job runs in parallel by default. This can be prevented by using the needs param to refer to another job that needs to be completed first. Create secrets in your Github repository to pass sensitive values to the action file. You can put them in Repository secrets in Github console.Example: Push to ECR &amp;amp; revise ECS Task Definitionon: push: branches: - mainname: Push image to ECR and revise ECS Task Definitionjobs: deploy: name: Deploy runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: $ aws-secret-access-key: $ aws-region: ca-central-1 - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v1 - name: Build, tag, and push image to Amazon ECR id: build-image env: ECR_REGISTRY: $ ECR_REPOSITORY: my-api-image IMAGE_TAG: $ run: | docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG -f ./backend/Dockerfile ./backend docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest echo &quot;image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG&quot; &amp;gt;&amp;gt; $GITHUB_OUTPUT - name: Download task definition run: | aws ecs describe-task-definition \\ --task-definition MyAPITaskDef \\ --query taskDefinition &amp;gt; task-definition.json - name: Fill in the new image ID in the Amazon ECS task definition id: task-def uses: aws-actions/amazon-ecs-render-task-definition@v1 with: task-definition: task-definition.json container-name: MyAPIContainer image: $ - name: Deploy Amazon ECS task definition uses: aws-actions/amazon-ecs-deploy-task-definition@v1 with: task-definition: $ service: MyAPIService cluster: MyAPICluster wait-for-service-stability: true" }, { "title": "ECS Fargate Task as a Web server", "url": "/posts/ecs-fargate-task-server/", "categories": "Guides", "tags": "deployment, aws, ecr, load-balancer, security-group, ecs, fargate, task, cloudformation", "date": "2023-12-03 22:42:00 +0600", "snippet": "Photo by C Dustin on UnsplashYou can manually run a one-off ECS Fargate Task based off a Dockerized image of your API server. But if you are expecting heavy traffic, you can setup Load Balancer, Service etc. In this guide we will setup everything using CloudFormation.Some key points to note are: we will create a Load Balancer with its own Security Group we will create a Service with its own Security Group (Tasks start automatically on creation of Service) we will modify Security Group (inbound rule) of Service to receive traffic from Security Group of Load Balancer When Task Definition is revised (essentially registering the task def again) with a new ECR image tag, the old Task running in the Service is stopped and its Public IP is stripped away. After a while the Task is removed. This Public IP was actually never directly consumed since the user accesses by the Load Balancer’s IP. Existing tasks and services that reference a DELETE_IN_PROGRESS task definition revision continue to run without disruption. If you stop a Task managed by a Service, a new instance of the Task will start again automatically if the Service has a target of running at least 1 Task at all times.PrerequisitesIn AWS Management Console, use the following settings to create a Security Group for the Load Balancer. Name: APILoadBalancerSG Inbound rule: Type: Custom TCP Protocol: TCP Port range: 80 Source: 0.0.0.0/0 Another SG for the ECS Service. Name: APIServiceSG Inbound rule: Type: Custom TCP Protocol: TCP Port range: 80 Source: APILoadBalancerSG These two SGs needed to be created beforehand because the API server in this example expected an RDS to preexist since alembic migrations are done on API task startup and the RDS instance needed a SG that referenced the APIServiceSG.CloudFormationAWSTemplateFormatVersion: 2010-09-09Parameters: DeploymentEnv: Description: The environment for deployment Type: String AllowedValues: - prod VPC: Type: AWS::EC2::VPC::Id SubnetA: Type: AWS::EC2::Subnet::Id SubnetB: Type: AWS::EC2::Subnet::Id ContainerSecurityGroup: # the APIServiceSG created earlier Type: AWS::EC2::SecurityGroup::Id LoadBalancerSecurityGroup: # the APILoadBalancerSG created earlier Type: AWS::EC2::SecurityGroup::IdResources: Cluster: Type: &#39;AWS::ECS::Cluster&#39; Properties: ClusterName: MyAPICluster CapacityProviders: - FARGATE TaskExecutionRole: Type: &#39;AWS::IAM::Role&#39; Properties: RoleName: MyAPITaskExecutionRole AssumeRolePolicyDocument: Version: &#39;2012-10-17&#39; Statement: - Effect: Allow Principal: Service: - ecs-tasks.amazonaws.com Action: - &#39;sts:AssumeRole&#39; Policies: - PolicyName: MyAPITaskExecutionRolePolicy PolicyDocument: Version: &quot;2012-10-17&quot; Statement: - Effect: Allow Action: - &#39;ecr:GetAuthorizationToken&#39; - &#39;ecr:BatchCheckLayerAvailability&#39; - &#39;ecr:GetDownloadUrlForLayer&#39; - &#39;ecr:BatchGetImage&#39; - &#39;logs:CreateLogStream&#39; - &#39;logs:PutLogEvents&#39; - &#39;ssm:DescribeParameters&#39; - &#39;ssm:GetParameterHistory&#39; - &#39;ssm:GetParametersByPath&#39; - &#39;ssm:GetParameters&#39; - &#39;ssm:GetParameter&#39; Resource: &#39;*&#39; TaskRole: Type: AWS::IAM::Role Properties: RoleName: MyAPITaskRole AssumeRolePolicyDocument: Statement: - Effect: Allow Principal: Service: ecs-tasks.amazonaws.com Action: &#39;sts:AssumeRole&#39; Policies: - PolicyName: MyAPITaskRolePolicy PolicyDocument: Version: &quot;2012-10-17&quot; Statement: - Sid: AllowSSM Effect: Allow Action: - &#39;ssm:DescribeParameters&#39; - &#39;ssm:GetParameterHistory&#39; - &#39;ssm:GetParametersByPath&#39; - &#39;ssm:GetParameters&#39; - &#39;ssm:GetParameter&#39; - &#39;ssm:PutParameter&#39; Resource: &#39;*&#39; - Sid: AllowLogs Effect: Allow Action: - &#39;logs:CreateLogStream&#39; - &#39;logs:PutLogEvents&#39; Resource: &#39;*&#39; - Sid: AllowECR Effect: Allow Action: - &#39;ecr:GetAuthorizationToken&#39; - &#39;ecr:BatchCheckLayerAvailability&#39; - &#39;ecr:GetDownloadUrlForLayer&#39; - &#39;ecr:BatchGetImage&#39; - &#39;ecr:DescribeImages&#39; - &#39;ecr:GetRepositoryPolicy&#39; - &#39;ecr:SetRepositoryPolicy&#39; Resource: - !Sub &#39;arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/*&#39; LogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: /ecs/MyAPITaskDef RetentionInDays: 30 TaskDefinition: Type: &#39;AWS::ECS::TaskDefinition&#39; DependsOn: - LogGroup - TaskExecutionRole - TaskRole Properties: Family: MyAPITaskDef ContainerDefinitions: - EntryPoint: - sh - &#39;-c&#39; Command: - &amp;gt;- /bin/bash -c &quot; alembic upgrade head; gunicorn src.main:app --workers 1 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:80 --log-level=debug --timeout=90&quot; Essential: true Image: !Join - &#39;&#39; - - !Ref AWS::AccountId - .dkr.ecr. - !Ref AWS::Region - .amazonaws.com/ - my-api-image:latest LogConfiguration: LogDriver: awslogs Options: awslogs-group: !Ref LogGroup awslogs-region: !Ref AWS::Region awslogs-stream-prefix: ecs Name: MyAPIContainer PortMappings: - ContainerPort: 80 Protocol: tcp Environment: - Name: ENV Value: !Ref DeploymentEnv Secrets: - Name: POSTGRES_HOST ValueFrom: !Sub &#39;/myapp/${DeploymentEnv}/db_host&#39; - Name: POSTGRES_DB ValueFrom: !Sub &#39;/myapp/${DeploymentEnv}/db_name&#39; - Name: POSTGRES_PASSWORD ValueFrom: !Sub &#39;/myapp/${DeploymentEnv}/db_password&#39; - Name: POSTGRES_PORT ValueFrom: !Sub &#39;/myapp/${DeploymentEnv}/db_port&#39; - Name: POSTGRES_USER ValueFrom: !Sub &#39;/myapp/${DeploymentEnv}/db_user&#39; - Name: AUTH0_DOMAIN ValueFrom: !Sub &#39;/myapp/${DeploymentEnv}/auth0/domain&#39; Cpu: 512 ExecutionRoleArn: !Ref TaskExecutionRole TaskRoleArn: !Ref TaskRole Memory: 1024 NetworkMode: awsvpc RequiresCompatibilities: - FARGATE LoadBalancer: Type: AWS::ElasticLoadBalancingV2::LoadBalancer Properties: LoadBalancerAttributes: - Key: idle_timeout.timeout_seconds Value: 60 Name: MyAPILoadBalancer Scheme: internet-facing SecurityGroups: - !Ref LoadBalancerSecurityGroup Subnets: - !Ref SubnetA - !Ref SubnetB TargetGroup: Type: AWS::ElasticLoadBalancingV2::TargetGroup Properties: HealthCheckIntervalSeconds: 30 HealthCheckPath: /v1/ping HealthCheckTimeoutSeconds: 6 UnhealthyThresholdCount: 2 HealthyThresholdCount: 2 Name: MyAPITargetGroup Port: 80 Protocol: HTTP TargetGroupAttributes: - Key: deregistration_delay.timeout_seconds Value: 60 TargetType: ip VpcId: !Ref VPC ListenerHTTP: Type: AWS::ElasticLoadBalancingV2::Listener Properties: DefaultActions: - TargetGroupArn: !Ref TargetGroup Type: forward LoadBalancerArn: !Ref LoadBalancer Port: 80 Protocol: HTTP Service: Type: &#39;AWS::ECS::Service&#39; DependsOn: - ListenerHTTP Properties: ServiceName: MyAPIService Cluster: !Ref Cluster DesiredCount: 1 HealthCheckGracePeriodSeconds: 60 LaunchType: FARGATE LoadBalancers: - ContainerName: MyAPIContainer ContainerPort: 80 TargetGroupArn: !Ref TargetGroup NetworkConfiguration: AwsvpcConfiguration: AssignPublicIp: ENABLED Subnets: - !Ref SubnetA - !Ref SubnetB SecurityGroups: - !Ref ContainerSecurityGroup TaskDefinition: !Ref TaskDefinitionOutputs: Endpoint: Description: Endpoint Value: !Join - &#39;&#39; - - http:// - !GetAtt LoadBalancer.DNSName" }, { "title": "Server side authentication with Auth0", "url": "/posts/auth0-server-side-authentication/", "categories": "Guides", "tags": "login, auth0, authentication, resource-owner-password, regular-web-app", "date": "2023-10-15 20:05:00 +0600", "snippet": " Auth0 can be daunting … at first.Photo by Micah Williams on UnsplashYour API Server handles registration and authenticationSo, you are coming from a traditional backend-handles-all-things-authentication scenario. Now you need to build something similar with Auth0. Well, the best thing to do is to go through Auth0 documentations with patience and familiarize yourself with their terminology (API, Application, various flows, etc.). But if you are still confused, this article may help confirm a few things.An M2M ApplicationStart with creating an API in the existing Tenant.A Database Connection is also created automatically.Alongwith the API, a Machine to Machine Application is automatically created which contains our domain, client_id and client_secret. In “Auth0 Dashboard” -&amp;gt; “Applications” -&amp;gt; “Applications” -&amp;gt; “YOUR_M2M_APP” -&amp;gt; “APIs”, you will find this M2M Application has been authorised to request access tokens for the API that we just created. But this application has not yet been authorized to request access tokens for the Auth0 Management API.Under these settings, you can set up the Auth0 SDK in the following manner (shown in Python with FastAPI). This will allow you to sign up users.# auth0.pyfrom auth0.authentication import Database, GetTokenfrom src.core.config import settingsdef auth0_authapi_database(): database = Database(settings.AUTH0_DOMAIN, settings.AUTH0_CLIENT_ID) return databasedef auth0_authapi_token(): token = GetToken( settings.AUTH0_DOMAIN, settings.AUTH0_CLIENT_ID, client_secret=settings.AUTH0_CLIENT_SECRET, ) return token# auth.pyfrom fastapi import APIRouter, Dependsfrom fastapi.responses import Responsefrom auth0.authentication import Database, GetTokenfrom src.core.config import settingsfrom src.deps.auth0 import auth0_authapi_database, auth0_authapi_tokenrouter = APIRouter()@router.post(&quot;/v1/auth/signup&quot;, tags=[&quot;auth&quot;])async def signup( auth0api_database: Database = Depends(auth0_authapi_database),) -&amp;gt; Response: response = auth0api_database.signup( email=&quot;ehsan_1@yopmail.com&quot;, password=&quot;Abc!_1234&quot;, connection=&quot;Username-Password-Authentication&quot;, ) return responseResource Owner Password FlowBut in order to use sign-in with username and password, the M2M Application needs to be given the password Grant Type in “Auth0 Dashboard” -&amp;gt; “Applications” -&amp;gt; “Applications” -&amp;gt; “YOUR_M2M_APP” -&amp;gt; “Settings” -&amp;gt; “Advanced Settings” -&amp;gt; “Grant Types”. This password Grant Type is what comprises a Resource Owner Password Flow. Now you can sign-in.# auth.py# ...@router.post(&quot;/v1/auth/signin&quot;, tags=[&quot;auth&quot;])async def signin( auth0api_token: GetToken = Depends(auth0_authapi_token),) -&amp;gt; Response: response = auth0api_token.login( username=&quot;ehsan_1@yopmail.com&quot;, password=&quot;Abc!_1234&quot;, realm=&quot;Username-Password-Authentication&quot;, audience=settings.AUTH0_API_AUDIENCE, ) return responseClient Credentials FlowBy default, an M2M only has the Client Credentials grant type. In the Client Credentials Flow, you just provide the Application client id and secret to obtain access tokens from Auth0. As such, the access token has no user information, it is an access token just for the Application itself authorised to use some API declared in Auth0. Hence, the term Machine to Machine.Authorization Code FlowNow, it will be ok to go forward with the M2M Application created for us and use its client_id to initialise the SDK. But if you have plans in the future for your API Server to be consumed by a frontend client where Authorization Code Flow might be adopted (client provides backend with an OAuth code, which the backend then exchanges with Auth0 for an access token), then you also need the Authorization Code Grant Type.A Regular Web AppBut instead of adding yet another Grant Type Authorization Code to our M2M application (which is only meant to have the Client Credentials grant type), we can create a new Application of type Regular Web App, which by default will have the Authorization Code and Client Credentials grant types amongst others. We need to add the password grant type manually here as well. So technically, any one of Resource Owner Password or Client Credentials or Authorization Code flows can be used with both M2M and Regular Web App type Applications.Granting Auth0 Management API permission to M2M ApplicationYou will also need the Auth0 Management API for user CRUD operations. In Auth0 Dashboard -&amp;gt; Applications -&amp;gt; APIs -&amp;gt; Auth0 Management API -&amp;gt; Machine To Machine Applications, first authorize your M2M application to request access tokens for the Management API. Then expand the M2M Application to also add the required permissions (scopes)." }, { "title": "Midnight appointments across timezones", "url": "/posts/midnight-appointments/", "categories": "Experiences", "tags": "timezone, UTC", "date": "2022-05-04 20:26:00 +0600", "snippet": " When one’s availability today translates to another’s tomorrow.Photo by Richard Tao on UnsplashPremiseA certain appointment scheduling feature allows a user (the host) to make themselves available during certain periods of their day in a week. Another user (the guest) from a different timezone wants to know the hours in their own timezone on which the host user is available. They may want to see these available hours for a date range, say from a Tuesday to a Thursday. Let’s think about the API I/O. Input: start date, end date (in the guest’s timezone) Output: a list of the appointment time slots that the host is available on (in the guest’s timezone)Setting up a use-caseThe host is in Los Angeles, CA, USA. The guest is in Dhaka, Bangladesh. The appointment duration is 1 hour. Suppose the host makes herself available from 0800 to 1200 (Pacific Standard Time, tz: America/Los_Angeles) on the following dates: March 29, 2022 March 30, 2022The guest in Dhaka (Asia/Dhaka, UTC+6) wants to know the host’s available hours on the following days of their own calendar: March 30, 2022Assuming Los Angeles is 14 hours behind Dhaka, our guest should see the following appointment slots for March 30, 2022 (UTC+6): 0000-0100 (equivalent to 1000-1100 in LA on March 29, 2022) 0100-0200 (equivalent to 1100-1200 in LA on March 29, 2022) 2200-2300 (equivalent to 0800-0900 in LA on March 30, 2022) 2300-2400 (equivalent to 0900-1000 in LA on March 30, 2022)The formulaThe guest sends the following input to the API: start_date: March 30, 2022 end_date: March 30, 2022The API needs to translate the given date range (in the guest’s timezone) to a date range in the host’s zone. The API subtracts or adds hours to the start_date of the guest depending upon whether the host is behind or ahead of the guest’s timezone. So, to the API, the start date becomes, in this case: March 30, 2022 at 0000 hours – 14 hours = March 29, 2022 at 1000 hours.For the end_date the formula is slightly different: March 30, 2022 at 0000 hours + 24 hours – 14 hours = March 30, 2022 at 1000 hours.So, now (in Pacific Standard Time, America/Los_Angeles) all that the API has to do is, find appointment slots between: March 29, 2022 at 1000 hours and March 30, 2022 at 1000 hours." }, { "title": "Jekyll, Chirpy and the Github Pages", "url": "/posts/gh-pages-jekyll-chirpy/", "categories": "Guides", "tags": "ruby, bundle, blog, blogging, host, hosting, jekyll, chirpy, theme, jekyll-theme, github, github-pages", "date": "2022-04-24 12:54:00 +0600", "snippet": " Github Pages lets you host Jekyll sites and Chirpy makes it even easier.Photo by Florian Bernhardt on UnsplashPrerequisitesMake sure to have Ruby installed.On Mac:moncefbelyamani’s tutorial.On Windowshttps://rubyinstaller.org/Steps Create a new repository from the Chirpy Starter and name it &amp;lt;GH_USERNAME&amp;gt;.github.io if you want to host with Github Pages at https://&amp;lt;GH_USERNAME&amp;gt;.github.io. If you want to host at https://&amp;lt;GH_USERNAME&amp;gt;.github.io/&amp;lt;REPO_NAME&amp;gt;, then just name the repo whatever you want. Cloned the repo locally. Installed dependencies with command bundle from both windows and mac. Also ran: bundle lock --add-platform x86_64-linux In _config.yml, updated baseurl to &amp;lt;REPO_NAME&amp;gt; updated url to https://&amp;lt;GH_USERNAME&amp;gt;.github.io Then pushed to main branch. That triggered the /.github/workflows/pages-deploy.yml and the branch gh-pages on github was automatically created and site build was also successful. Then went to repository on GitHub &amp;gt; Settings &amp;gt; Pages and as Source, chose gh-pages as the branch and /(root) as the root folder. Shortly afterwards, the site became accessible.Launch locallybundle exec jekyll serveCustom domainGitHub Pages can be configured with a custom domain. GitHub’s documentations are pretty comprehensive and easy to follow, but let’s go over some of the key steps that GitHub will make you perform in order to configure a custom domain. Verify domain: GitHub will want to know that you really own the custom domain that you claim to own. This is accomplished by GH providing you with a TXT record details that you must create on your DNS provider. Add domain to repo in GH: On your GH repository page, go to “Settings” &amp;gt; “Code and automation” &amp;gt; “Pages” &amp;gt; “Custom domain”, type your custom domain, then click Save. Since I was publishing my site from a branch, this created a commit that added a CNAME file directly to the root of my source branch (gh-pages). Create A records on your DNS provider: Time to create some A records on your DNS provider. These A records will point your apex custom domain (YOURDOMAIN.COM) to GitHub Pages server IP addresses. Be sure to delete any existing A records that might point to some other IP addresses. Check if it worked: dig YOURDOMAIN.COM +noall +answer -t A Redirect subdomain: You can redirect a subdomain like www by creating a CNAME record on your DNS provider so that the subdomain points to &amp;lt;GH_USERNAME&amp;gt;.github.io. Check if it worked: dig WWW.YOURDOMAIN.COM +nostats +nocomments +nocmd If you want to host directly at https://YOURDOMAIN.COM, then no need to rename the repo to &amp;lt;GH_USERNAME&amp;gt;.github.io. Only in _config.yml, set baseurl to &#39;&#39;. " }, { "title": "Maximizing asynchronous OCR operations with Tesseract and Quartz", "url": "/posts/async-tesseract-quartz/", "categories": "Experiences", "tags": "tesseract-ocr, OCR, concurrency, quartz, java, multi-threading, scheduler, job", "date": "2019-12-12 17:55:00 +0600", "snippet": "Photo by Sear Greyson on UnsplashThe ObjectiveThe application has to let a user upload multiple PDFs together. And on each of those PDFs, an Optical Character Recognition (OCR) operation needs to be run to glean photos and some information.Restrictions The application is deployed on-premise, on a machine that is restricted from connecting to the internet while the software functions. The machine is limited to 4 CPU cores.Tesseract and the catchWe decided to use Tesseract to help us out with the OCR. Our server application was written in Spring (Java), from which a wrapper (Tess4j) would invoke the tesseract-ocr engine.Our original plan was to let tesseract-ocr manage its own multithreading to get a PDF OCRed as quickly as possible, and then move on to the next one in the queue of uploaded PDFs. But tesseract-ocr in multithread mode was significantly slower than in single-thread mode at the time this application was being made.So we forced each spawned process of tesseract-ocr to use one thread only by setting OMP_THREAD_LIMIT=1 in the environment. But now, it would be great if we could launch 4 of those processes together to get through the PDFs faster.Quartz the SchedulerQuartz allows us to create jobs and then run those jobs concurrently if needed. So, every time a PDF was successfully uploaded synchronously at the request of the user, we scheduled a job for it. This asynchronous job would actually invoke the tesseract-ocr. When done with a PDF, the job updates a record on our database so that the user can learn about the OCR completion.We told Quartz to keep it to 4 concurrent jobs at maximum. And this combination of single-threaded Tesseract and a multi-threaded Quartz, was the sweet spot for our application." } ]
